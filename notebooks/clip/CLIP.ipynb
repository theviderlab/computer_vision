{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Interacting with CLIP.ipynb",
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPHN7PJgKOzb"
      },
      "source": [
        "# CLIP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53N4k0pj_9qL"
      },
      "source": [
        "## Instalar de librerías"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0BpdJkdBssk9"
      },
      "source": [
        "%%capture\n",
        "!pip install ftfy regex tqdm\n",
        "!pip install git+https://github.com/openai/CLIP.git"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importar de librerías"
      ],
      "metadata": {
        "id": "0vVmn-w1sFWS"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C1hkDT38hSaP"
      },
      "source": [
        "import numpy as np\n",
        "import clip\n",
        "import os\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from google.colab import drive"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive/', force_remount=True)\n",
        "BASE_FOLDER = '/content/drive/My Drive/Colab/CLIP/'\n",
        "os.chdir(BASE_FOLDER)\n",
        "\n",
        "DATASET_PATH = os.path.join(BASE_FOLDER, 'datasets')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4vT_5eVW-z72",
        "outputId": "c0abad7e-ec43-4d25-808f-140bd85b223b"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFxgLV5HAEEw"
      },
      "source": [
        "## Cargar el modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uLFS29hnhlY4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd6cda4f-0468-44d9-bb67-b29b935c5eb4"
      },
      "source": [
        "clip.available_models()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['RN50',\n",
              " 'RN101',\n",
              " 'RN50x4',\n",
              " 'RN50x16',\n",
              " 'RN50x64',\n",
              " 'ViT-B/32',\n",
              " 'ViT-B/16',\n",
              " 'ViT-L/14',\n",
              " 'ViT-L/14@336px']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IBRVTY9lbGm8"
      },
      "source": [
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model, preprocess_clip = clip.load(\"ViT-B/32\", device=DEVICE)\n",
        "if DEVICE == \"cuda\":\n",
        "  model.cuda().eval();\n",
        "else:\n",
        "  model.cpu().eval();"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21slhZGCqANb"
      },
      "source": [
        "## Preprocesamiento\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imágenes"
      ],
      "metadata": {
        "id": "pyLs2xlyxJy8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Resolución de las imágenes:\", model.visual.input_resolution)"
      ],
      "metadata": {
        "id": "QlCf3sP4xGty"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6cpiIFHp9N6"
      },
      "source": [
        "preprocess_clip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Textos"
      ],
      "metadata": {
        "id": "ZNEslD5NxMy2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Longitud del contexto:\", model.context_length)"
      ],
      "metadata": {
        "id": "LBW4VQ3RxFXV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qGom156-i2kL"
      },
      "source": [
        "clip.tokenize(\"Hola mundo!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4W8ARJVqBJXs"
      },
      "source": [
        "## Clasificador zero-shot"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_texts(dataset):\n",
        "  text_descriptions = [f\"This is a photo of a {label}\" for label in dataset.classes]\n",
        "  text_tokens = clip.tokenize(text_descriptions).to(DEVICE)\n",
        "  return text_tokens"
      ],
      "metadata": {
        "id": "fSoZCXd6mSfe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_images(dataset, indexes):\n",
        "  images = []\n",
        "  labels = []\n",
        "  original_images = []\n",
        "\n",
        "  for idx in indexes:\n",
        "    image, label = dataset[idx]\n",
        "    label_name = dataset.classes[label]\n",
        "    original_images.append((image, label_name))\n",
        "\n",
        "    # Preprocesar la imagen para CLIP\n",
        "    image_input = preprocess_clip(image).unsqueeze(0).to(DEVICE)\n",
        "    images.append(image_input)\n",
        "    labels.append(label)\n",
        "\n",
        "  return (images, labels, original_images)"
      ],
      "metadata": {
        "id": "0ZBoTp1Xl9Oz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def show_example(dataset, index):\n",
        "  image, label = dataset[index]\n",
        "  plt.imshow(image)\n",
        "  plt.title(f\"Etiqueta: {dataset.classes[label]}\\n Resolución: {image.size}\")\n",
        "  plt.axis('off')\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "LKy2wQvHnqQH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def show_predictions(dataset, original_images, predictions):\n",
        "  # Mostrar las imágenes con sus predicciones\n",
        "  plt.figure(figsize=(15, 5))\n",
        "  for i in range(len(original_images)):\n",
        "    image, label_name = original_images[i]\n",
        "    predicted_class = dataset.classes[predictions[i].argmax()]\n",
        "\n",
        "    # Mostrar la imagen\n",
        "    plt.subplot(1, 3, i + 1)\n",
        "    plt.imshow(image)\n",
        "    plt.title(f\"Etiqueta real: {label_name}\\nPredicción: {predicted_class}\")\n",
        "    plt.axis('off')\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "6-QiaKGWuzJh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_torch_dataset(dataset_library, dataset_path, dataset_folder):\n",
        "  # Ruta específica donde el dataset almacena sus archivos\n",
        "  dataset_folder = os.path.join(dataset_path, dataset_folder)\n",
        "\n",
        "  # Ruta del archivo .tar.gz descargado\n",
        "  dataset_tar = os.path.join(dataset_path, dataset_folder + '.tar.gz')\n",
        "\n",
        "  # Verificar si el dataset ya está descargado\n",
        "  if not os.path.exists(dataset_folder):\n",
        "      print(\"Descargando dataset...\")\n",
        "      dataset = dataset_library(root=dataset_path, transform=None, download=True)\n",
        "      print(\"Descarga completa.\")\n",
        "\n",
        "      # Después de la descarga, eliminar el archivo .tar.gz\n",
        "      if os.path.exists(dataset_tar):\n",
        "          os.remove(dataset_tar)\n",
        "          print(f\"Archivo {dataset_tar} eliminado.\")\n",
        "  else:\n",
        "      print(\"El dataset ya está descargado.\")\n",
        "      dataset = dataset_library(root=dataset_path, transform=None, download=False)\n",
        "\n",
        "  return dataset"
      ],
      "metadata": {
        "id": "K6RdJ5IWuBju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CIFAR100"
      ],
      "metadata": {
        "id": "vYboDW2jKBYB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.datasets import CIFAR100\n",
        "cifar100 = load_torch_dataset(CIFAR100, DATASET_PATH, 'cifar-100-python')"
      ],
      "metadata": {
        "id": "tuxOkVqZt8Gg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_example(cifar100, 1)"
      ],
      "metadata": {
        "id": "cc9y2dFgzkEL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preparar las descripciones de texto y tokenizarlas\n",
        "text_tokens = preprocess_texts(cifar100)\n",
        "\n",
        "with torch.no_grad():\n",
        "    text_features = model.encode_text(text_tokens).float()\n",
        "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "print(\"Longitud del contexto:\", text_tokens[0].shape)\n",
        "print(\"Vectores de textos:\", text_features[:3])\n",
        "print(\"Tamaño del espacio vectorial para el texto:\", text_features[0].shape)"
      ],
      "metadata": {
        "id": "N-KpMDHAuGnW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Seleccionar 3 índices aleatorios\n",
        "indexes = random.sample(range(len(cifar100)), 3)\n",
        "\n",
        "images, labels, original_images = preprocess_images(cifar100, indexes)\n",
        "\n",
        "# Concatenar las imágenes en un solo tensor\n",
        "image_inputs = torch.cat(images, dim=0)\n",
        "\n",
        "with torch.no_grad():\n",
        "  image_features = model.encode_image(image_inputs).float()\n",
        "  image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "print(\"Resolución de las imágenes:\", image_inputs[0].shape)\n",
        "print(\"Tamaño del espacio vectorial para las imágenes:\", image_features[0].shape)"
      ],
      "metadata": {
        "id": "ltUl_4oGudXH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    # Calcular similitudes manualmente\n",
        "    similarities = image_features @ text_features.T\n",
        "    predictions = similarities.softmax(dim=-1).cpu().numpy()\n",
        "\n",
        "print(\"Formato de predicciones:\", predictions.shape)\n",
        "print(\"Probabilidad acumulada:\", predictions[0].sum())"
      ],
      "metadata": {
        "id": "1gruOq7Dw_Xb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_predictions(cifar100, original_images, predictions)"
      ],
      "metadata": {
        "id": "h1N2qDmvtxLy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### STL10"
      ],
      "metadata": {
        "id": "4doEgvScz6fd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.datasets import STL10"
      ],
      "metadata": {
        "id": "HC7tB7r7yDZF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargar el conjunto de datos STL10 sin transformaciones\n",
        "stl10 = load_torch_dataset(STL10, DATASET_PATH, 'stl10_binary')"
      ],
      "metadata": {
        "id": "QqW7A_JgyHqW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_example(stl10, 1)"
      ],
      "metadata": {
        "id": "E6Ct0lbjv3Ha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_tokens = preprocess_texts(stl10)"
      ],
      "metadata": {
        "id": "NaIoPqBJyJ22"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Seleccionar 3 índices aleatorios\n",
        "indexes = random.sample(range(len(stl10)), 3)\n",
        "\n",
        "images, labels, original_images = preprocess_images(stl10, indexes)\n",
        "\n",
        "# Concatenar las imágenes en un solo tensor\n",
        "image_inputs = torch.cat(images, dim=0)"
      ],
      "metadata": {
        "id": "pM9pMYIkyOKN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    # Calcular logits y obtener probabilidades\n",
        "    logits_per_image, _ = model(image_inputs, text_tokens)\n",
        "    predictions = logits_per_image.softmax(dim=-1).cpu().numpy()"
      ],
      "metadata": {
        "id": "GCPOz0rNyRPA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_predictions(stl10, original_images, predictions)"
      ],
      "metadata": {
        "id": "YxXWLb2AyTxE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Buscar objetos\n",
        "\n"
      ],
      "metadata": {
        "id": "c1pOR_J_18Ev"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -P \"$DATASET_PATH\" https://github.com/theviderlab/computer_vision/raw/main/notebooks/clip/hidden-objects.jpg"
      ],
      "metadata": {
        "id": "LN3UpQWUxnN5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_path = os.path.join(DATASET_PATH, 'hidden-objects.jpg')\n",
        "image = Image.open(image_path)\n",
        "\n",
        "width, height = image.size\n",
        "print(image.size)"
      ],
      "metadata": {
        "id": "N_GdluOs2DaL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(20, 10))\n",
        "plt.imshow(image)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "G5Cgc2rh0hxg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_region(x, y, patch_dim, window, image):\n",
        "  left = x * patch_dim\n",
        "  upper = y * patch_dim\n",
        "  right = left + window * patch_dim\n",
        "  lower = upper + window * patch_dim\n",
        "\n",
        "  # Extraer la región de la imagen\n",
        "  region = image.crop((left, upper, right, lower))\n",
        "\n",
        "  return region"
      ],
      "metadata": {
        "id": "ZtH2SJbqIQOx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir el tamaño de los parches\n",
        "patch_dim = 64\n",
        "\n",
        "# Calcular el número de parches en cada dimensión\n",
        "num_patches_x = width // patch_dim\n",
        "num_patches_y = height // patch_dim\n",
        "\n",
        "# Definir la ventana\n",
        "window = 3"
      ],
      "metadata": {
        "id": "Gc_uzE7W2HV5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "region = get_region(3, 5, patch_dim, window, image)\n",
        "plt.imshow(region)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TfpDR8NQVPFC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preparar el texto (en inglés para mejores resultados)\n",
        "text_prompt = \"image of a spider\"\n",
        "text_tokens = clip.tokenize([text_prompt]).to(DEVICE)"
      ],
      "metadata": {
        "id": "chuANlVS6Alb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Matriz para almacenar las probabilidades\n",
        "pred_matrix = np.zeros((num_patches_x - window + 1, num_patches_y - window + 1))\n",
        "pred_matrix.shape"
      ],
      "metadata": {
        "id": "8XU4_K2U6CvQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Recorrer la imagen con una ventana deslizante\n",
        "for x in range(pred_matrix.shape[0]):\n",
        "    for y in range(pred_matrix.shape[1]):\n",
        "\n",
        "        # Extraer la región de la imagen\n",
        "        region = get_region(x, y, patch_dim, window, image)\n",
        "\n",
        "        # Preprocesar la región para CLIP\n",
        "        image_input = preprocess_clip(region).unsqueeze(0).to(DEVICE)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # Calcular logits y obtener la similitud directamente\n",
        "            logits_per_image, _ = model(image_input, text_tokens)\n",
        "            similarity = logits_per_image.squeeze().cpu().item()\n",
        "\n",
        "        # Almacenar la similitud en la matriz\n",
        "        pred_matrix[x, y] = similarity"
      ],
      "metadata": {
        "id": "e0rs9XOv6XaU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizar el mapa de probabilidades entre 0 y 1\n",
        "prediction = (pred_matrix - pred_matrix.min()) / (pred_matrix.max() - pred_matrix.min())\n",
        "prediction[prediction < 0.8] = 0.1"
      ],
      "metadata": {
        "id": "qGRxRE7KLHv5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mostrar la matriz de probabilidades como una imagen\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.imshow(prediction.T, cmap='hot', interpolation='nearest')\n",
        "plt.colorbar(label=\"Similitud\")\n",
        "plt.title(f\"Mapa de predicciones de regiones con '{text_prompt}'\")\n",
        "plt.xlabel('Posición X de la ventana')\n",
        "plt.ylabel('Posición Y de la ventana')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "E3v0k9kC6g4E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Redimensionar el mapa de probabilidades al tamaño de la imagen\n",
        "from scipy.ndimage import zoom\n",
        "\n",
        "# Calcular el factor de ampliación\n",
        "zoom_factor_x = width / prediction.shape[0]\n",
        "zoom_factor_y = height / prediction.shape[1]\n",
        "\n",
        "# Ampliar el mapa de probabilidades\n",
        "prediction_map = zoom(prediction, (zoom_factor_x, zoom_factor_y), order=1)\n",
        "prediction_map = prediction_map[:width, :height]\n",
        "prediction_map = np.stack([prediction_map.T]*3, axis=-1)\n",
        "\n",
        "# Modificar la intensidad de la imagen original\n",
        "image_modulated = np.array(image)\n",
        "image_modulated = image_modulated.astype(np.float32) * prediction_map\n",
        "image_modulated = np.clip(image_modulated, 0, 255).astype(np.uint8)\n",
        "image_modulated = Image.fromarray(image_modulated)\n",
        "\n",
        "# Mostrar la imagen original y la imagen modulada\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(image)\n",
        "plt.title('Imagen Original')\n",
        "plt.axis('off')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.imshow(image_modulated)\n",
        "plt.title(f'Imagen Modulada según \"{text_prompt}\"')\n",
        "plt.axis('off')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "PHZ7PXCj7mnP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_top_patches = 3\n",
        "\n",
        "flatten_patches = prediction.flatten()\n",
        "sorted_patches = np.argsort(flatten_patches)\n",
        "top_patches = sorted_patches[-num_top_patches:]\n",
        "coordinates = np.unravel_index(top_patches, prediction.shape)\n",
        "\n",
        "# Extraer y mostrar los parches correspondientes\n",
        "plt.figure(figsize=(15, 6))\n",
        "for idx in range(num_top_patches):\n",
        "    # Extraer la región de la imagen\n",
        "    x = coordinates[0][idx]\n",
        "    y = coordinates[1][idx]\n",
        "    region = get_region(x, y, patch_dim, window, image)\n",
        "\n",
        "    # Mostrar el parche\n",
        "    plt.subplot(2, 5, idx + 1)\n",
        "    plt.imshow(region)\n",
        "    plt.title(f\"Similitud: {prediction[x, y]:.2f}\")\n",
        "    plt.axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rflzomXeYx9a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-tuning"
      ],
      "metadata": {
        "id": "KXh-go7Fj27B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Instalar Kaggle"
      ],
      "metadata": {
        "id": "76m7lDqPka1r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade --force-reinstall --no-deps kaggle"
      ],
      "metadata": {
        "id": "9wGw0sQm7_VA",
        "outputId": "61754f5d-2d46-40cb-9509-d3cf400d9c08",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting kaggle\n",
            "  Using cached kaggle-1.6.17-py3-none-any.whl\n",
            "Installing collected packages: kaggle\n",
            "  Attempting uninstall: kaggle\n",
            "    Found existing installation: kaggle 1.6.17\n",
            "    Uninstalling kaggle-1.6.17:\n",
            "      Successfully uninstalled kaggle-1.6.17\n",
            "Successfully installed kaggle-1.6.17\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Descargar dataset"
      ],
      "metadata": {
        "id": "CEtcU4K_khTE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir las variables necesarias\n",
        "KAGGLE_USER = \"kursatkomurcu\"\n",
        "KAGGLE_DATASET = \"satellite-image-caption-change-detection\"\n",
        "dataset_origin = f\"{KAGGLE_USER}/{KAGGLE_DATASET}\"\n",
        "dataset_folder = os.path.join(DATASET_PATH, KAGGLE_DATASET.replace('-', '_'))\n",
        "dataset_zip = os.path.join(DATASET_PATH, KAGGLE_DATASET + '.zip')\n",
        "kaggle_json_path = os.path.join(DATASET_PATH, 'kaggle.json')  # Corregido el nombre\n",
        "\n",
        "# Verificar que el archivo kaggle.json existe\n",
        "if not os.path.exists(kaggle_json_path):\n",
        "    raise FileNotFoundError(f\"El archivo {kaggle_json_path} no existe. Por favor, colócalo en esa ubicación.\")\n",
        "\n",
        "# Verificar si el dataset ya está descargado\n",
        "if not os.path.exists(dataset_folder):\n",
        "\n",
        "  # Descargar el dataset\n",
        "  if not os.path.exists(dataset_zip):\n",
        "    print(\"El dataset no está descargado. Procediendo a descargarlo...\")\n",
        "\n",
        "    # Configurar las credenciales de Kaggle\n",
        "    !mkdir -p ~/.kaggle\n",
        "    !cp \"{kaggle_json_path}\" ~/.kaggle/\n",
        "    !chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "    !kaggle datasets download -d \"{dataset_origin}\" -p \"{DATASET_PATH}\"\n",
        "\n",
        "  # Eliminar el archivo comprimido\n",
        "  if os.path.exists(dataset_zip):\n",
        "    # Descomprimir el archivo\n",
        "    !unzip -o \"{dataset_zip}\" -d \"{DATASET_PATH}\" > /dev/null\n",
        "\n",
        "    os.remove(dataset_zip)\n",
        "    print(f\"Archivo {dataset_zip} eliminado.\")\n",
        "else:\n",
        "  print(\"El dataset ya está descargado.\")"
      ],
      "metadata": {
        "id": "4hUBCoYe8hBE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f6adcb0-3e0b-4688-f40d-9c3ca49616ac"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "El dataset ya está descargado.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import files\n",
        "\n",
        "# from torchvision import transforms\n",
        "\n",
        "# from transformers import CLIPProcessor, CLIPModel"
      ],
      "metadata": {
        "id": "WPOztI4T9Uty"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_names = ['CLCD', 'DSIFN', 'LEVIR-CD', 'S2Looking']\n",
        "\n",
        "caption_path = os.path.join(dataset_folder, 'captions')\n",
        "caption_train_path = os.path.join(caption_path, 'train')\n",
        "caption_val_path = os.path.join(caption_path, 'val')"
      ],
      "metadata": {
        "id": "t2C3wKejTO8j"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataframes = []\n",
        "\n",
        "for dataset_i in dataset_names:\n",
        "    # Construir la ruta al archivo CSV\n",
        "    csv_path = os.path.join(caption_train_path, 'train_' + dataset_i + '.csv')\n",
        "    # Leer el CSV en un DataFrame\n",
        "    captions_df = pd.read_csv(csv_path)\n",
        "    # Añadir el DataFrame a la lista\n",
        "    dataframes.append(captions_df)\n",
        "\n",
        "# Concatenar todos los DataFrames en uno solo\n",
        "captions_train_df = pd.concat(dataframes, ignore_index=True)\n",
        "\n",
        "captions_train_df['img_path'] = dataset_folder + '/' + dataset_i + '/train/' + captions_train_df['img']\n",
        "captions_train_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 379
        },
        "id": "ArkBzigUTqCl",
        "outputId": "ce4ae389-7251-466e-e9e0-879256260e8a"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         img  change                                           caption1  \\\n",
              "0  00000.png       1  The satellite image depicts a rural area with ...   \n",
              "1  00001.png       1  The satellite image depicts a rural area with ...   \n",
              "2  00002.png       1  The satellite image depicts a rural landscape ...   \n",
              "3  00003.png       0  The satellite image depicts a rural landscape ...   \n",
              "4  00004.png       1  The satellite image depicts a rural village su...   \n",
              "\n",
              "                                            caption2  \\\n",
              "0  The satellite image shows a village surrounded...   \n",
              "1  The satellite image shows a rural area with pa...   \n",
              "2  The satellite image depicts a rural area with ...   \n",
              "3  The satellite image depicts a rural area with ...   \n",
              "4  The satellite image depicts a rural landscape,...   \n",
              "\n",
              "                                            img_path  \n",
              "0  /content/drive/My Drive/Colab/CLIP/datasets/sa...  \n",
              "1  /content/drive/My Drive/Colab/CLIP/datasets/sa...  \n",
              "2  /content/drive/My Drive/Colab/CLIP/datasets/sa...  \n",
              "3  /content/drive/My Drive/Colab/CLIP/datasets/sa...  \n",
              "4  /content/drive/My Drive/Colab/CLIP/datasets/sa...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e41b1b1d-ea15-4ac6-b595-09dea5c5c2e9\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>img</th>\n",
              "      <th>change</th>\n",
              "      <th>caption1</th>\n",
              "      <th>caption2</th>\n",
              "      <th>img_path</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>00000.png</td>\n",
              "      <td>1</td>\n",
              "      <td>The satellite image depicts a rural area with ...</td>\n",
              "      <td>The satellite image shows a village surrounded...</td>\n",
              "      <td>/content/drive/My Drive/Colab/CLIP/datasets/sa...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>00001.png</td>\n",
              "      <td>1</td>\n",
              "      <td>The satellite image depicts a rural area with ...</td>\n",
              "      <td>The satellite image shows a rural area with pa...</td>\n",
              "      <td>/content/drive/My Drive/Colab/CLIP/datasets/sa...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>00002.png</td>\n",
              "      <td>1</td>\n",
              "      <td>The satellite image depicts a rural landscape ...</td>\n",
              "      <td>The satellite image depicts a rural area with ...</td>\n",
              "      <td>/content/drive/My Drive/Colab/CLIP/datasets/sa...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>00003.png</td>\n",
              "      <td>0</td>\n",
              "      <td>The satellite image depicts a rural landscape ...</td>\n",
              "      <td>The satellite image depicts a rural area with ...</td>\n",
              "      <td>/content/drive/My Drive/Colab/CLIP/datasets/sa...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>00004.png</td>\n",
              "      <td>1</td>\n",
              "      <td>The satellite image depicts a rural village su...</td>\n",
              "      <td>The satellite image depicts a rural landscape,...</td>\n",
              "      <td>/content/drive/My Drive/Colab/CLIP/datasets/sa...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e41b1b1d-ea15-4ac6-b595-09dea5c5c2e9')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e41b1b1d-ea15-4ac6-b595-09dea5c5c2e9 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e41b1b1d-ea15-4ac6-b595-09dea5c5c2e9');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-4559f22b-bad2-4e51-aead-ddf124489a6f\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-4559f22b-bad2-4e51-aead-ddf124489a6f')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-4559f22b-bad2-4e51-aead-ddf124489a6f button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "captions_train_df",
              "summary": "{\n  \"name\": \"captions_train_df\",\n  \"rows\": 14555,\n  \"fields\": [\n    {\n      \"column\": \"img\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 12738,\n        \"samples\": [\n          \"4656.png\",\n          \"4867.png\",\n          \"779.jpg\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"change\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"caption1\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 14552,\n        \"samples\": [\n          \"The satellite image depicts a landscape with varied terrain, likely captured by a satellite for the purpose of geographical analysis or environmental monitoring. The different colors and textures suggest a mix of natural features such as forests, agricultural lands, and bodies of water. The presence of roads indicates human habitation or activity in the area. This type of imagery is commonly used to study land use patterns, environmental changes, and to aid in planning and decision-making processes related to land management and resource allocation.\",\n          \"The satellite image depicts a landscape with various land and water features. It includes a body of water, likely a lake or reservoir, surrounded by different shades of green areas that could be fields or agricultural land. There are also patches of dark green, which might indicate dense vegetation or forested areas. The presence of roads suggests accessibility and human activity in the region. Overall, it is a rural area with a mix of natural and man-made elements.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"caption2\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 14536,\n        \"samples\": [\n          \"The satellite image depicts a developed urban area with various types of land use. It shows a mix of residential and commercial zones, as evidenced by the presence of buildings and roads. The blue areas are likely water bodies or open spaces, such as parks or sports fields. The green areas could be parks, gardens, or undeveloped land. The image is valuable for understanding the layout, zoning, and development patterns within the city.\",\n          \"The satellite image provided is a grayscale aerial photograph of an urban area. It shows a mix of residential and commercial zones, with buildings varying in size and shape. The roads are clearly visible as lighter lines amidst the darker patches of built-up areas and open spaces. There's no color information available due to the grayscale nature of the image. This type of imagery is typically used for urban planning, infrastructure development, and environmental monitoring. To provide a detailed description, one would need to analyze the layout of the streets, the distribution of buildings, and the presence of any notable landmarks or features that could indicate the location or the purpose of the area.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"img_path\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 12738,\n        \"samples\": [\n          \"/content/drive/My Drive/Colab/CLIP/datasets/satellite_image_caption_change_detection/S2Looking/train/4656.png\",\n          \"/content/drive/My Drive/Colab/CLIP/datasets/satellite_image_caption_change_detection/S2Looking/train/4867.png\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_encoded = clip.tokenize(captions_train_df['caption1'][0], truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 438
        },
        "id": "rg2gstn_gs80",
        "outputId": "1bae7b8f-765f-482d-85ba-e42e761b6858"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Input The satellite image depicts a rural area with several distinct features. The presence of the water body suggests a natural landscape, potentially used for irrigation or as a source of drinking water for the nearby population. The clustered buildings with dark roofs indicate human habitation and possibly agricultural activity, as the dark roofs could be indicative of thatched or tiled roofing materials commonly found in rural settings. The road provides connectivity to the village, allowing for the transport of goods and people. The surrounding green areas suggest fertile land, which could be used for farming. Overall, this image portrays a self-sustaining rural community with a balance between human settlement and natural environment. is too long for context length 77",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-6b8a7fa88ae4>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtext_encoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcaptions_train_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'caption1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/clip/clip.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(texts, context_length, truncate)\u001b[0m\n\u001b[1;32m    240\u001b[0m                 \u001b[0mtokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meot_token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 242\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Input {texts[i]} is too long for context length {context_length}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    243\u001b[0m         \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Input The satellite image depicts a rural area with several distinct features. The presence of the water body suggests a natural landscape, potentially used for irrigation or as a source of drinking water for the nearby population. The clustered buildings with dark roofs indicate human habitation and possibly agricultural activity, as the dark roofs could be indicative of thatched or tiled roofing materials commonly found in rural settings. The road provides connectivity to the village, allowing for the transport of goods and people. The surrounding green areas suggest fertile land, which could be used for farming. Overall, this image portrays a self-sustaining rural community with a balance between human settlement and natural environment. is too long for context length 77"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "captions_train_df['splitted_captions'] = captions_train_df['caption1'].str.split(pat='.') + \\\n",
        "                                         captions_train_df['caption2'].str.split(pat='.')\n",
        "\n",
        "splitted_captions_train_df = captions_train_df.explode('splitted_captions').reset_index(drop=True)\n",
        "splitted_captions_train_df = splitted_captions_train_df.loc[:,['img_path', 'splitted_captions']]\n",
        "splitted_captions_train_df = splitted_captions_train_df.drop_duplicates()\n"
      ],
      "metadata": {
        "id": "MMf44dz8aABo"
      },
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_paths_train = splitted_captions_train_df['img_path'].to_numpy()\n",
        "captions_train = splitted_captions_train_df['splitted_captions'].to_numpy()"
      ],
      "metadata": {
        "id": "83Zk6ZJp_XGg"
      },
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if DEVICE == \"gpu\":\n",
        "  model.cuda().train();\n",
        "else:\n",
        "  model.cpu().train();"
      ],
      "metadata": {
        "id": "h7__uKiZlQcH"
      },
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the CLIP model and processor\n",
        "# model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "# processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")"
      ],
      "metadata": {
        "id": "GTEoCO3S_ZbP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pre-trained CLIP model\n",
        "# model, preprocess = clip.load(\"ViT-B/32\", device=DEVICE, jit=False)"
      ],
      "metadata": {
        "id": "9qUXFNdc_ibq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing pipeline (similar to CLIP's preprocessing)\n",
        "# transform = transforms.Compose([\n",
        "#     transforms.Resize((224, 224)),  # Resize images to 224x224 (standard for ViT models)\n",
        "#     transforms.ToTensor(),\n",
        "#     transforms.Normalize(mean=(0.48145466, 0.4578275, 0.40821073), std=(0.26862954, 0.26130258, 0.27577711))\n",
        "# ])"
      ],
      "metadata": {
        "id": "F1BnGapMEwt0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ImageTitleDataset(Dataset):\n",
        "    def __init__(self, img_paths, captions, transform=None):\n",
        "        self.image_paths = img_paths\n",
        "        self.captions = captions\n",
        "        self.transform = transform\n",
        "        self.tokenized_captions = clip.tokenize(captions, truncate=True)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.image_paths[idx]\n",
        "        image = Image.open(img_path).convert(\"RGB\")  # Open image and ensure it's RGB\n",
        "        if self.transform:\n",
        "            image = self.transform(image)  # Apply transformations\n",
        "        caption = self.tokenized_captions[idx]\n",
        "        return image, caption"
      ],
      "metadata": {
        "id": "lkTcxXJl_o6g"
      },
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to convert model's parameters to FP32 format\n",
        "# def convert_models_to_fp32(model):\n",
        "#     for p in model.parameters():\n",
        "#         p.data = p.data.float()\n",
        "#         if p.grad is not None:\n",
        "#             p.grad.data = p.grad.data.float()"
      ],
      "metadata": {
        "id": "XmH5Dw8KASOt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the custom dataset that pairs images with their titles\n",
        "dataset = ImageTitleDataset(img_paths_train, captions_train, transform=preprocess_clip)\n",
        "\n",
        "# Use a DataLoader to load images in batches to save memory\n",
        "train_dataloader = DataLoader(dataset, batch_size=128, shuffle=True)"
      ],
      "metadata": {
        "id": "Nngy9ai2_yZp"
      },
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if DEVICE == \"cpu\":\n",
        "    model.float()\n",
        "\n",
        "# Prepare the optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=5e-5, betas=(0.9, 0.98), eps=1e-6, weight_decay=0.2)\n",
        "\n",
        "# Specify the loss function\n",
        "loss_img = nn.CrossEntropyLoss()\n",
        "loss_txt = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "QsXZ8WulAUle"
      },
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "num_epochs = 15\n",
        "for epoch in range(num_epochs):\n",
        "    pbar = tqdm(train_dataloader, total=len(train_dataloader))\n",
        "    for batch in pbar:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        images, texts = batch\n",
        "\n",
        "        images = images.to(DEVICE)\n",
        "        texts = texts.to(DEVICE)\n",
        "\n",
        "        # Forward pass\n",
        "        logits_per_image, logits_per_text = model(images, texts)\n",
        "\n",
        "        # Compute loss\n",
        "        ground_truth = torch.arange(len(images), dtype=torch.long, device=DEVICE)\n",
        "        total_loss = (loss_img(logits_per_image, ground_truth) + loss_txt(logits_per_text, ground_truth)) / 2\n",
        "\n",
        "        # Backward pass\n",
        "        total_loss.backward()\n",
        "        if DEVICE == \"cpu\":\n",
        "            optimizer.step()\n",
        "        else:\n",
        "            convert_models_to_fp32(model)\n",
        "            optimizer.step()\n",
        "            clip.model.convert_weights(model)\n",
        "\n",
        "        pbar.set_description(f\"Epoch {epoch}/{num_epochs}, Loss: {total_loss.item():.4f}\")"
      ],
      "metadata": {
        "id": "pe_KwIog5tu0",
        "outputId": "19e4d1bd-62b6-4e68-b868-9b409accf6d8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 0/30, Loss: 2.5898: 100%|██████████| 2849/2849 [15:25<00:00,  3.08it/s]\n",
            "Epoch 1/30, Loss: 1.8896: 100%|██████████| 2849/2849 [14:45<00:00,  3.22it/s]\n",
            "Epoch 2/30, Loss: 2.0625: 100%|██████████| 2849/2849 [14:45<00:00,  3.22it/s]\n",
            "Epoch 3/30, Loss: 2.3203: 100%|██████████| 2849/2849 [14:42<00:00,  3.23it/s]\n",
            "Epoch 4/30, Loss: 2.1289: 100%|██████████| 2849/2849 [14:43<00:00,  3.23it/s]\n",
            "Epoch 5/30, Loss: 2.0566: 100%|██████████| 2849/2849 [14:42<00:00,  3.23it/s]\n",
            "Epoch 6/30, Loss: 2.2090: 100%|██████████| 2849/2849 [14:39<00:00,  3.24it/s]\n",
            "Epoch 7/30, Loss: 1.9854: 100%|██████████| 2849/2849 [14:37<00:00,  3.25it/s]\n",
            "Epoch 8/30, Loss: 2.1016: 100%|██████████| 2849/2849 [14:38<00:00,  3.24it/s]\n",
            "Epoch 9/30, Loss: 2.5000: 100%|██████████| 2849/2849 [14:38<00:00,  3.24it/s]\n",
            "Epoch 10/30, Loss: 2.0312: 100%|██████████| 2849/2849 [14:46<00:00,  3.22it/s]\n",
            "Epoch 11/30, Loss: 2.4844: 100%|██████████| 2849/2849 [15:11<00:00,  3.13it/s]\n",
            "Epoch 12/30, Loss: 1.7227: 100%|██████████| 2849/2849 [15:32<00:00,  3.05it/s]\n",
            "Epoch 13/30, Loss: 2.6777: 100%|██████████| 2849/2849 [15:32<00:00,  3.06it/s]\n",
            "Epoch 14/30, Loss: 1.9697: 100%|██████████| 2849/2849 [14:40<00:00,  3.24it/s]\n",
            "Epoch 15/30, Loss: 2.6484: 100%|██████████| 2849/2849 [14:34<00:00,  3.26it/s]\n",
            "Epoch 16/30, Loss: 2.1973: 100%|██████████| 2849/2849 [14:32<00:00,  3.26it/s]\n",
            "Epoch 17/30, Loss: 2.4863: 100%|██████████| 2849/2849 [14:32<00:00,  3.27it/s]\n",
            "Epoch 18/30, Loss: 2.5391: 100%|██████████| 2849/2849 [14:30<00:00,  3.27it/s]\n",
            "Epoch 19/30, Loss: 2.1035: 100%|██████████| 2849/2849 [14:28<00:00,  3.28it/s]\n",
            "Epoch 20/30, Loss: 2.8047: 100%|██████████| 2849/2849 [14:29<00:00,  3.28it/s]\n",
            "Epoch 21/30, Loss: 2.3906: 100%|██████████| 2849/2849 [14:25<00:00,  3.29it/s]\n",
            "Epoch 22/30, Loss: 2.7129: 100%|██████████| 2849/2849 [14:25<00:00,  3.29it/s]\n",
            "Epoch 23/30, Loss: 2.5879: 100%|██████████| 2849/2849 [14:24<00:00,  3.30it/s]\n",
            "Epoch 24/30, Loss: 2.7676: 100%|██████████| 2849/2849 [14:24<00:00,  3.30it/s]\n",
            "Epoch 25/30, Loss: 2.7344: 100%|██████████| 2849/2849 [14:26<00:00,  3.29it/s]\n",
            "Epoch 26/30, Loss: 2.3359: 100%|██████████| 2849/2849 [14:25<00:00,  3.29it/s]\n",
            "Epoch 27/30, Loss: 2.4570: 100%|██████████| 2849/2849 [14:25<00:00,  3.29it/s]\n",
            "Epoch 28/30, Loss: 2.5664: 100%|██████████| 2849/2849 [14:29<00:00,  3.28it/s]\n",
            "Epoch 29/30, Loss: 2.4238:  92%|█████████▏| 2627/2849 [13:17<01:05,  3.39it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Guardar los pesos del modelo después del entrenamiento\n",
        "save_path = '/content/modelo_entrenado.pth'\n",
        "torch.save(model.state_dict(), save_path)\n",
        "print(f\"Pesos del modelo guardados en {save_path}\")"
      ],
      "metadata": {
        "id": "pzHYrGqMlKsm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}